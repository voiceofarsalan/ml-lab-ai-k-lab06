# Lab 06 â€“ PCA and Dimensionality Reduction | AI K

This lab focuses on implementing **Principal Component Analysis (PCA)** to perform dimensionality reduction on multivariate datasets, aiming to improve computational efficiency and preserve key patterns in data.

## ðŸ§  Objective
- Understand and apply PCA for reducing data dimensions.
- Visualize how PCA impacts data variance and distribution.
- Evaluate how dimensionality reduction affects model performance.

## ðŸ“‚ Contents
- Loading and exploring high-dimensional datasets.
- Standardizing data to prepare for PCA.
- Applying `sklearn.decomposition.PCA` to perform dimensionality reduction.
- Analyzing the explained variance to choose optimal number of components.
- Visualizing PCA output in 2D/3D plots.
- Running classification on reduced dimensions to compare performance.

## ðŸ§° Libraries & Tools Used
- `pandas` & `numpy` â€“ for data manipulation
- `matplotlib` & `seaborn` â€“ for visualization
- `sklearn.preprocessing.StandardScaler` â€“ for feature scaling
- `sklearn.decomposition.PCA` â€“ for dimensionality reduction
- `sklearn.model_selection` & `sklearn.metrics` â€“ for evaluation

## âœ… Outcomes
- Implemented PCA on sample datasets and reduced feature space.
- Visualized the cumulative variance to determine components needed.
- Demonstrated how dimensionality reduction simplifies data without losing essential variance.

---

